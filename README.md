# ğŸš— Big Data Analysis with PySpark â€“ US Accidents (2016â€“2023)

## ğŸ“Œ Objective

This project demonstrates scalable big data processing using **PySpark** to analyze a large dataset of traffic accidents in the United States. The dataset contains over 7 million records from 2016 to 2023 and is used to extract meaningful insights about accident frequency, severity, timing, and locations.

---

## ğŸ“ Dataset Details

- **Name**: US Accidents (Dec 2023)
- **Source**: [Kaggle - US Accidents Dataset](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents)
- **Format**: CSV
- **Size**: ~1.2 GB
- **Records**: 7.5+ million
- **Key Columns**:
  - `Start_Time`, `End_Time`
  - `City`, `State`, `Severity`
  - `Weather_Condition`, `Temperature(F)`, `Visibility(mi)`

---

## âš™ï¸ Tools & Technologies

- **PySpark** â€“ for distributed data processing
- **Python 3.8+**
- **Jupyter Notebook / .py script**
- (Optional) Google Colab / Databricks / AWS EMR

---

## ğŸš€ Setup Instructions

1. **Install PySpark**:
   ```bash
   pip install pyspark
